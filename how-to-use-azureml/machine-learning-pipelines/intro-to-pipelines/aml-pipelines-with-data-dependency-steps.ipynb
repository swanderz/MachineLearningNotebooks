{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Copyright (c) Microsoft Corporation. All rights reserved.  \n",
        "Licensed under the MIT License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/MachineLearningNotebooks/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-data-dependency-steps.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Azure Machine Learning Pipelines with Data Dependency\n",
        "In this notebook, we will see how we can build a pipeline with implicit data dependency."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisites and Azure Machine Learning Basics\n",
        "If you are using an Azure Machine Learning Notebook VM, you are all set. Otherwise, make sure you go through the [configuration Notebook](https://aka.ms/pl-config) first if you haven't. This sets you up with a working config file that has information on your workspace, subscription id, etc. \n",
        "\n",
        "### Azure Machine Learning and Pipeline SDK-specific Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "SDK version: 1.1.5\nPipeline SDK-specific imports completed\n"
        }
      ],
      "source": [
        "import azureml.core\n",
        "from azureml.core import Workspace, Experiment, Datastore\n",
        "from azureml.core.compute import AmlCompute\n",
        "from azureml.core.compute import ComputeTarget\n",
        "from azureml.widgets import RunDetails\n",
        "\n",
        "# Check core SDK version number\n",
        "print(\"SDK version:\", azureml.core.VERSION)\n",
        "\n",
        "from azureml.data.data_reference import DataReference\n",
        "from azureml.pipeline.core import Pipeline, PipelineData\n",
        "from azureml.pipeline.steps import PythonScriptStep\n",
        "print(\"Pipeline SDK-specific imports completed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Initialize Workspace\n",
        "\n",
        "Initialize a [workspace](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.workspace(class%29) object from persisted configuration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "tags": [
          "create workspace"
        ]
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "avadevitsmlsvc\nRG-ITSMLTeam-Dev\nwestus2\nff2e23ae-7d7c-4cbd-99b8-116bb94dca6e\nBlobstore's name: workspaceblobstore\n"
        }
      ],
      "source": [
        "ws = Workspace.from_config()\n",
        "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep = '\\n')\n",
        "\n",
        "# Default datastore (Azure blob storage)\n",
        "# def_blob_store = ws.get_default_datastore()\n",
        "def_blob_store = Datastore(ws, \"workspaceblobstore\")\n",
        "print(\"Blobstore's name: {}\".format(def_blob_store.name))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Source Directory\n",
        "The best practice is to use separate folders for scripts and its dependent files for each step and specify that folder as the `source_directory` for the step. This helps reduce the size of the snapshot created for the step (only the specific folder is snapshotted). Since changes in any files in the `source_directory` would trigger a re-upload of the snapshot, this helps keep the reuse of the step when there are no changes in the `source_directory` of the step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Sample scripts will be created in data_dependency_run_train directory.\n"
        }
      ],
      "source": [
        "# source directory\n",
        "source_directory = 'data_dependency_run_train'\n",
        "    \n",
        "print('Sample scripts will be created in {} directory.'.format(source_directory))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Required data and script files for the the tutorial\n",
        "Sample files required to finish this tutorial are already copied to the project folder specified above. Even though the .py provided in the samples don't have much \"ML work,\" as a data scientist, you will work on this extensively as part of your work. To complete this tutorial, the contents of these files are not very important. The one-line files are for demostration purpose only."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Compute Targets\n",
        "See the list of Compute Targets on the workspace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# cts = ws.compute_targets\n",
        "# for ct in cts:\n",
        "#     print(ct)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Retrieve or create an Aml compute\n",
        "Azure Machine Learning Compute is a service for provisioning and managing clusters of Azure virtual machines for running machine learning workloads. Let's get the default Aml Compute in the current workspace. We will then run the training script on this compute target."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "found existing compute target.\nAml Compute attached\n"
        }
      ],
      "source": [
        "from azureml.core.compute_target import ComputeTargetException\n",
        "\n",
        "aml_compute_target = \"cpu-cluster\"\n",
        "try:\n",
        "    aml_compute = AmlCompute(ws, aml_compute_target)\n",
        "    print(\"found existing compute target.\")\n",
        "except ComputeTargetException:\n",
        "    print(\"creating new compute target\")\n",
        "    \n",
        "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = \"STANDARD_D2_V2\",\n",
        "                                                                min_nodes = 1, \n",
        "                                                                max_nodes = 4)    \n",
        "    aml_compute = ComputeTarget.create(ws, aml_compute_target, provisioning_config)\n",
        "    aml_compute.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
        "    \n",
        "print(\"Aml Compute attached\")\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# For a more detailed view of current Azure Machine Learning Compute status, use get_status()\n",
        "# example: un-comment the following line.\n",
        "# print(aml_compute.get_status().serialize())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Wait for this call to finish before proceeding (you will see the asterisk turning to a number).**\n",
        "\n",
        "Now that you have created the compute target, let's see what the workspace's compute_targets() function returns. You should now see one entry named 'amlcompute' of type AmlCompute."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Building Pipeline Steps with Inputs and Outputs\n",
        "As mentioned earlier, a step in the pipeline can take data as input. This data can be a data source that lives in one of the accessible data locations, or intermediate data produced by a previous step in the pipeline.\n",
        "\n",
        "### Datasources\n",
        "Datasource is represented by **[DataReference](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.data.data_reference.datareference?view=azure-ml-py)** object and points to data that lives in or is accessible from Datastore. DataReference could be a pointer to a file or a directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "DataReference object created\n"
        }
      ],
      "source": [
        "# Reference the data uploaded to blob storage using DataReference\n",
        "# Assign the datasource to blob_input_data variable\n",
        "\n",
        "# DataReference(datastore, \n",
        "#               data_reference_name=None, \n",
        "#               path_on_datastore=None, \n",
        "#               mode='mount', \n",
        "#               path_on_compute=None, \n",
        "#               overwrite=False)\n",
        "\n",
        "blob_input_data = DataReference(\n",
        "    datastore=def_blob_store,\n",
        "    data_reference_name=\"test_data\",\n",
        "    path_on_datastore=\"20newsgroups/20news.pkl\")\n",
        "print(\"DataReference object created\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Intermediate/Output Data\n",
        "Intermediate data (or output of a Step) is represented by **[PipelineData](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-core/azureml.pipeline.core.pipelinedata?view=azure-ml-py)** object. PipelineData can be produced by one step and consumed in another step by providing the PipelineData object as an output of one step and the input of one or more steps.\n",
        "\n",
        "#### Constructing PipelineData\n",
        "- **name:** [*Required*] Name of the data item within the pipeline graph\n",
        "- **datastore_name:** Name of the Datastore to write this output to\n",
        "- **output_name:** Name of the output\n",
        "- **output_mode:** Specifies \"upload\" or \"mount\" modes for producing output (default: mount)\n",
        "- **output_path_on_compute:** For \"upload\" mode, the path to which the module writes this output during execution\n",
        "- **output_overwrite:** Flag to overwrite pre-existing data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "PipelineData object created\n"
        }
      ],
      "source": [
        "# Define intermediate data using PipelineData\n",
        "# Syntax\n",
        "\n",
        "# PipelineData(name, \n",
        "#              datastore=None, \n",
        "#              output_name=None, \n",
        "#              output_mode='mount', \n",
        "#              output_path_on_compute=None, \n",
        "#              output_overwrite=None, \n",
        "#              data_type=None, \n",
        "#              is_directory=None)\n",
        "\n",
        "# Naming the intermediate data as processed_data1 and assigning it to the variable processed_data1.\n",
        "processed_data1 = PipelineData(\"processed_data1\",datastore=def_blob_store)\n",
        "print(\"PipelineData object created\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Pipelines steps using datasources and intermediate data\n",
        "Machine learning pipelines can have many steps and these steps could use or reuse datasources and intermediate data. Here's how we construct such a pipeline:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Define a Step that consumes a datasource and produces intermediate data.\n",
        "In this step, we define a step that consumes a datasource and produces intermediate data.\n",
        "\n",
        "**Open `train.py` in the local machine and examine the arguments, inputs, and outputs for the script. That will give you a good sense of why the script argument names used below are important.** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Specify conda dependencies and a base docker image through a RunConfiguration\n",
        "\n",
        "This step uses a docker image and scikit-learn, use a [**RunConfiguration**](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.runconfiguration?view=azure-ml-py) to specify these requirements and use when creating the PythonScriptStep. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azureml.core.runconfig import RunConfiguration\n",
        "from azureml.core.conda_dependencies import CondaDependencies\n",
        "from azureml.core.runconfig import DEFAULT_CPU_IMAGE\n",
        "\n",
        "# create a new runconfig object\n",
        "run_config = RunConfiguration()\n",
        "\n",
        "# enable Docker \n",
        "run_config.environment.docker.enabled = True\n",
        "\n",
        "# set Docker base image to the default CPU-based image\n",
        "run_config.environment.docker.base_image = DEFAULT_CPU_IMAGE\n",
        "\n",
        "# use conda_dependencies.yml to create a conda environment in the Docker image for execution\n",
        "run_config.environment.python.user_managed_dependencies = False\n",
        "\n",
        "# specify CondaDependencies obj\n",
        "cd = CondaDependencies()\n",
        "cd.add_pip_package(\"azureml-sdk==1.1.5.1\")\n",
        "cd.add_conda_package(\"scikit-learn\")\n",
        "run_config.environment.python.conda_dependencies = cd\n",
        "."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "trainStep created\n"
        }
      ],
      "source": [
        "# step4 consumes the datasource (Datareference) in the previous step\n",
        "# and produces processed_data1\n",
        "trainStep = PythonScriptStep(\n",
        "    script_name=\"train.py\", \n",
        "    arguments=[\"--input_data\", blob_input_data, \"--output_train\", processed_data1],\n",
        "    inputs=[blob_input_data],\n",
        "    outputs=[processed_data1],\n",
        "    compute_target=aml_compute, \n",
        "    source_directory='data_dependency_run_train',\n",
        "    runconfig=run_config\n",
        ")\n",
        "print(\"trainStep created\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Define a Step that consumes intermediate data and produces intermediate data\n",
        "In this step, we define a step that consumes an intermediate data and produces intermediate data.\n",
        "\n",
        "**Open `extract.py` in the local machine and examine the arguments, inputs, and outputs for the script. That will give you a good sense of why the script argument names used below are important.** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "extractStep created\n"
        }
      ],
      "source": [
        "# step5 to use the intermediate data produced by step4\n",
        "# This step also produces an output processed_data2\n",
        "processed_data2 = PipelineData(\"processed_data2\", datastore=def_blob_store)\n",
        "\n",
        "extractStep = PythonScriptStep(\n",
        "    script_name=\"extract.py\",\n",
        "    arguments=[\"--input_extract\", processed_data1, \"--output_extract\", processed_data2],\n",
        "    inputs=[processed_data1],\n",
        "    outputs=[processed_data2],\n",
        "    compute_target=aml_compute, \n",
        "    source_directory=\"data_dependency_run_extract\")\n",
        "print(\"extractStep created\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Define a Step that consumes intermediate data and existing data and produces intermediate data\n",
        "In this step, we define a step that consumes multiple data types and produces intermediate data.\n",
        "\n",
        "This step uses the output generated from the previous step as well as existing data on a DataStore. The location of the existing data is specified using a [**PipelineParameter**](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-core/azureml.pipeline.core.pipelineparameter?view=azure-ml-py) and a [**DataPath**](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.data.datapath.datapath?view=azure-ml-py). Using a PipelineParameter enables easy modification of the data location when the Pipeline is published and resubmitted.\n",
        "\n",
        "**Open `compare.py` in the local machine and examine the arguments, inputs, and outputs for the script. That will give you a good sense of why the script argument names used below are important.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reference the data uploaded to blob storage using a PipelineParameter and a DataPath\n",
        "from azureml.pipeline.core import PipelineParameter\n",
        "from azureml.data.datapath import DataPath, DataPathComputeBinding\n",
        "\n",
        "datapath = DataPath(datastore=def_blob_store, path_on_datastore='20newsgroups/20news.pkl')\n",
        "datapath_param = PipelineParameter(name=\"compare_data\", default_value=datapath)\n",
        "data_parameter1 = (datapath_param, DataPathComputeBinding(mode='mount'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "compareStep created\n"
        }
      ],
      "source": [
        "# Now define the compare step which takes two inputs and produces an output\n",
        "processed_data3 = PipelineData(\"processed_data3\", datastore=def_blob_store) \n",
        "\n",
        "compareStep = PythonScriptStep(\n",
        "    script_name=\"compare.py\",\n",
        "    arguments=[\"--compare_data1\", data_parameter1, \"--compare_data2\", processed_data2, \"--output_compare\", processed_data3],\n",
        "    inputs=[data_parameter1, processed_data2],\n",
        "    outputs=[processed_data3],    \n",
        "    compute_target=aml_compute, \n",
        "    source_directory=\"data_dependency_run_compare\")\n",
        "print(\"compareStep created\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Build the pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Pipeline is built\n"
        }
      ],
      "source": [
        "pipeline1 = Pipeline(workspace=ws, steps=[compareStep])\n",
        "print (\"Pipeline is built\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Created step compare.py [4eb1e450][ec685585-9209-4816-9f50-2e4c74d69d29], (This step will run and generate new outputs)Created step extract.py [7eebcf8a][588b4d70-5038-4826-8a39-aec5b180396f], (This step is eligible to reuse a previous run's output)\n\nCreated step train.py [47f0edc3][f78ea68c-cca1-4cc3-9c07-ad75efac8ecc], (This step will run and generate new outputs)\nCreated data reference workspaceblobstore_ede545fc for StepId [363eec69][d1bd701f-b596-4c7c-bb4c-8293687740cc], (Consumers of this data will generate new runs.)\nUsing data reference test_data for StepId [7d5d55f1][e412565a-205a-4edd-b57e-57ac47b14acd], (Consumers of this data are eligible to reuse prior runs.)\nSubmitted PipelineRun 107dde6d-3721-477a-98bf-5f791b7bc78f\nLink to Azure Machine Learning Portal: https://ml.azure.com/experiments/Data_dependency/runs/107dde6d-3721-477a-98bf-5f791b7bc78f?wsid=/subscriptions/ff2e23ae-7d7c-4cbd-99b8-116bb94dca6e/resourcegroups/RG-ITSMLTeam-Dev/workspaces/avadevitsmlsvc\nPipeline is submitted for execution\n"
        }
      ],
      "source": [
        "pipeline_run1 = Experiment(ws, 'Data_dependency').submit(pipeline1)\n",
        "print(\"Pipeline is submitted for execution\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "_PipelineWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', â€¦",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bd1c66146f8e4fcdb2f980a86d1ac0a4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/aml.mini.widget.v1": "{\"loading\": true}"
          },
          "metadata": {}
        }
      ],
      "source": [
        "RunDetails(pipeline_run1).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Wait for pipeline run to complete"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "tags": [
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend"
        ]
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "=====================================================================\nbash: /azureml-envs/azureml_1b417bb747e35859ebf611fb43071e9c/lib/libtinfo.so.5: no version information available (required by bash)\nStarting job release. Current time:2020-03-18T21:29:47.801424\nLogging experiment finalizing status in history service.\nStarting the daemon thread to refresh tokens in background for process with pid = 167\nJob release is complete. Current time:2020-03-18T21:29:50.213118\n\nStepRun(extract.py) Execution Summary\n======================================\nStepRun( extract.py ) Status: Finished\n{'runId': '8739d44e-4316-4b53-a97f-3a12717fb6ce', 'target': 'cpu-cluster', 'status': 'Completed', 'startTimeUtc': '2020-03-18T21:29:18.143663Z', 'endTimeUtc': '2020-03-18T21:30:04.421659Z', 'properties': {'azureml.runsource': 'azureml.StepRun', 'ContentSnapshotId': 'ce62991d-a5ae-4041-a146-142a76a6ee56', 'StepType': 'PythonScriptStep', 'ComputeTargetType': 'AmlCompute', 'azureml.pipelinerunid': '107dde6d-3721-477a-98bf-5f791b7bc78f', '_azureml.ComputeTargetType': 'amlcompute', 'ProcessInfoFile': 'azureml-logs/process_info.json', 'ProcessStatusFile': 'azureml-logs/process_status.json'}, 'inputDatasets': [], 'runDefinition': {'script': 'extract.py', 'useAbsolutePath': False, 'arguments': ['--input_extract', '$AZUREML_DATAREFERENCE_processed_data1', '--output_extract', '$AZUREML_DATAREFERENCE_processed_data2'], 'sourceDirectoryDataStore': None, 'framework': 'Python', 'communicator': 'None', 'target': 'cpu-cluster', 'dataReferences': {'processed_data1': {'dataStoreName': 'workspaceblobstore', 'mode': 'Mount', 'pathOnDataStore': 'azureml/62490b92-efe4-4a2f-8fd4-8a77e10fede5/processed_data1', 'pathOnCompute': None, 'overwrite': False}, 'processed_data2': {'dataStoreName': 'workspaceblobstore', 'mode': 'Mount', 'pathOnDataStore': 'azureml/8739d44e-4316-4b53-a97f-3a12717fb6ce/processed_data2', 'pathOnCompute': None, 'overwrite': False}}, 'data': {}, 'jobName': None, 'maxRunDurationSeconds': None, 'nodeCount': 1, 'environment': {'name': 'Experiment Data_dependency Environment', 'version': 'Autosave_2020-02-06T19:50:52Z_809dae5e', 'python': {'interpreterPath': 'python', 'userManagedDependencies': False, 'condaDependencies': {'channels': ['conda-forge'], 'dependencies': ['python=3.6.2', {'pip': ['azureml-defaults']}], 'name': 'azureml_1b417bb747e35859ebf611fb43071e9c'}, 'baseCondaEnvironment': None}, 'environmentVariables': {'EXAMPLE_ENV_VAR': 'EXAMPLE_VALUE'}, 'docker': {'baseImage': 'mcr.microsoft.com/azureml/base:intelmpi2018.3-ubuntu16.04', 'baseDockerfile': None, 'baseImageRegistry': {'address': None, 'username': None, 'password': None}, 'enabled': True, 'shmSize': '1g'}, 'spark': {'repositories': ['[]'], 'packages': [], 'precachePackages': True}, 'inferencingStackVersion': None}, 'history': {'outputCollection': True, 'directoriesToWatch': ['logs']}, 'spark': {'configuration': {'spark.app.name': 'Azure ML Experiment', 'spark.yarn.maxAppAttempts': '1'}}, 'amlCompute': {'name': None, 'vmSize': None, 'retainCluster': False, 'clusterMaxNodeCount': 1}, 'tensorflow': {'workerCount': 1, 'parameterServerCount': 1}, 'mpi': {'processCountPerNode': 1}, 'hdi': {'yarnDeployMode': 'Cluster'}, 'containerInstance': {'region': None, 'cpuCores': 2, 'memoryGb': 3.5}, 'exposedPorts': None, 'docker': {'useDocker': True, 'sharedVolumes': True, 'shmSize': '2g', 'arguments': []}, 'cmk8sCompute': {'configuration': {}}}, 'logFiles': {'azureml-logs/55_azureml-execution-tvmps_a908d1ef2dcb3755096ff07a574681d0b0400a48aae14b143627456f0ee35f11_d.txt': 'https://avadevitsmlsvc7139228118.blob.core.windows.net/azureml/ExperimentRun/dcid.8739d44e-4316-4b53-a97f-3a12717fb6ce/azureml-logs/55_azureml-execution-tvmps_a908d1ef2dcb3755096ff07a574681d0b0400a48aae14b143627456f0ee35f11_d.txt?sv=2019-02-02&sr=b&sig=mBPoS3WoZHRe0M5FatyPkqjVyestClZK0Kwg%2F4WOa6M%3D&st=2020-03-18T21%3A20%3A29Z&se=2020-03-19T05%3A30%3A29Z&sp=r', 'azureml-logs/65_job_prep-tvmps_a908d1ef2dcb3755096ff07a574681d0b0400a48aae14b143627456f0ee35f11_d.txt': 'https://avadevitsmlsvc7139228118.blob.core.windows.net/azureml/ExperimentRun/dcid.8739d44e-4316-4b53-a97f-3a12717fb6ce/azureml-logs/65_job_prep-tvmps_a908d1ef2dcb3755096ff07a574681d0b0400a48aae14b143627456f0ee35f11_d.txt?sv=2019-02-02&sr=b&sig=mZanwF8Bz7qhs4VlaZIVRIMYk%2FnhQbknh7GR0SKR8yA%3D&st=2020-03-18T21%3A20%3A29Z&se=2020-03-19T05%3A30%3A29Z&sp=r', 'azureml-logs/70_driver_log.txt': 'https://avadevitsmlsvc7139228118.blob.core.windows.net/azureml/ExperimentRun/dcid.8739d44e-4316-4b53-a97f-3a12717fb6ce/azureml-logs/70_driver_log.txt?sv=2019-02-02&sr=b&sig=3y0co%2F1eaifSUqSIvVOQAHSJ8hLEJbhsgf9cBBypZ2A%3D&st=2020-03-18T21%3A20%3A29Z&se=2020-03-19T05%3A30%3A29Z&sp=r', 'azureml-logs/75_job_post-tvmps_a908d1ef2dcb3755096ff07a574681d0b0400a48aae14b143627456f0ee35f11_d.txt': 'https://avadevitsmlsvc7139228118.blob.core.windows.net/azureml/ExperimentRun/dcid.8739d44e-4316-4b53-a97f-3a12717fb6ce/azureml-logs/75_job_post-tvmps_a908d1ef2dcb3755096ff07a574681d0b0400a48aae14b143627456f0ee35f11_d.txt?sv=2019-02-02&sr=b&sig=YkHSMGCHkB7V3A9%2F9c4mzm6d%2F6pHa5dg9G5vOMgRmM8%3D&st=2020-03-18T21%3A20%3A29Z&se=2020-03-19T05%3A30%3A29Z&sp=r', 'azureml-logs/process_info.json': 'https://avadevitsmlsvc7139228118.blob.core.windows.net/azureml/ExperimentRun/dcid.8739d44e-4316-4b53-a97f-3a12717fb6ce/azureml-logs/process_info.json?sv=2019-02-02&sr=b&sig=KXL%2FpWJz5jbSls9dF3%2BXjNWFesoJ6%2Frfm00zBgcYDVk%3D&st=2020-03-18T21%3A20%3A29Z&se=2020-03-19T05%3A30%3A29Z&sp=r', 'azureml-logs/process_status.json': 'https://avadevitsmlsvc7139228118.blob.core.windows.net/azureml/ExperimentRun/dcid.8739d44e-4316-4b53-a97f-3a12717fb6ce/azureml-logs/process_status.json?sv=2019-02-02&sr=b&sig=CK5SDvvlG5V64LU3XiyK8PPRRTCrOv%2BMGEDhLabBZA4%3D&st=2020-03-18T21%3A20%3A29Z&se=2020-03-19T05%3A30%3A29Z&sp=r', 'logs/azureml/141_azureml.log': 'https://avadevitsmlsvc7139228118.blob.core.windows.net/azureml/ExperimentRun/dcid.8739d44e-4316-4b53-a97f-3a12717fb6ce/logs/azureml/141_azureml.log?sv=2019-02-02&sr=b&sig=tLICCGXn289Mj6NyOKHAeMb3JrzzVh%2Be%2B9AODsKo1DY%3D&st=2020-03-18T21%3A20%3A29Z&se=2020-03-19T05%3A30%3A29Z&sp=r', 'logs/azureml/azureml.log': 'https://avadevitsmlsvc7139228118.blob.core.windows.net/azureml/ExperimentRun/dcid.8739d44e-4316-4b53-a97f-3a12717fb6ce/logs/azureml/azureml.log?sv=2019-02-02&sr=b&sig=L8l6N%2Br2iAm%2F5JxcLuqCGrlZw6t%2BbdbtAMsMC0TS%2B%2FY%3D&st=2020-03-18T21%3A20%3A29Z&se=2020-03-19T05%3A30%3A29Z&sp=r', 'logs/azureml/executionlogs.txt': 'https://avadevitsmlsvc7139228118.blob.core.windows.net/azureml/ExperimentRun/dcid.8739d44e-4316-4b53-a97f-3a12717fb6ce/logs/azureml/executionlogs.txt?sv=2019-02-02&sr=b&sig=XFBdgNcDo033bdoVgXlbqjzmpusAt1iw4XQYIBcykPg%3D&st=2020-03-18T21%3A20%3A29Z&se=2020-03-19T05%3A30%3A29Z&sp=r', 'logs/azureml/stderrlogs.txt': 'https://avadevitsmlsvc7139228118.blob.core.windows.net/azureml/ExperimentRun/dcid.8739d44e-4316-4b53-a97f-3a12717fb6ce/logs/azureml/stderrlogs.txt?sv=2019-02-02&sr=b&sig=GvdDx5WxckOwRZf5tegMKVSwYJ5AY%2FSU%2B4E%2FzwCmel8%3D&st=2020-03-18T21%3A20%3A29Z&se=2020-03-19T05%3A30%3A29Z&sp=r', 'logs/azureml/stdoutlogs.txt': 'https://avadevitsmlsvc7139228118.blob.core.windows.net/azureml/ExperimentRun/dcid.8739d44e-4316-4b53-a97f-3a12717fb6ce/logs/azureml/stdoutlogs.txt?sv=2019-02-02&sr=b&sig=OkTg4y%2BjmP2QUB8KKjWQJKNqp1Gnlpl0%2BayIShOJvIo%3D&st=2020-03-18T21%3A20%3A29Z&se=2020-03-19T05%3A30%3A29Z&sp=r'}}\n\n\n\n\nStepRunId: 813385a4-ab91-452d-89a6-e58bbda14cbd\nLink to Azure Machine Learning Portal: https://ml.azure.com/experiments/Data_dependency/runs/813385a4-ab91-452d-89a6-e58bbda14cbd?wsid=/subscriptions/ff2e23ae-7d7c-4cbd-99b8-116bb94dca6e/resourcegroups/RG-ITSMLTeam-Dev/workspaces/avadevitsmlsvc\nStepRun( compare.py ) Status: NotStarted\nStepRun( compare.py ) Status: Queued\nStepRun( compare.py ) Status: Running\n\nStreaming azureml-logs/55_azureml-execution-tvmps_a908d1ef2dcb3755096ff07a574681d0b0400a48aae14b143627456f0ee35f11_d.txt\n========================================================================================================================\n2020-03-18T21:31:20Z Starting output-watcher...\n2020-03-18T21:31:20Z IsDedicatedCompute == True, won't poll for Low Pri Preemption\nLogin Succeeded\nUsing default tag: latest\nlatest: Pulling from azureml/azureml_bbf736b212d7e6227e70e54a9e4b7f44\nDigest: sha256:abf56a3a11e96c70afd92a4b28dfb607b52714968cb95d5d032cf843313c4cb8\nStatus: Image is up to date for viennaglobal.azurecr.io/azureml/azureml_bbf736b212d7e6227e70e54a9e4b7f44:latest\na101574e97677705721a7912ec7b3f64e8c4a967d00e71172419851681e228f3\n2020/03/18 21:31:23 Version: 3.0.01160.0001 Branch: master Commit: 6b1e6e76\n2020/03/18 21:31:23 /dev/infiniband/uverbs0 found (implying presence of InfiniBand)?: false\n2020/03/18 21:31:23 sshd runtime has already been installed in the container\nssh-keygen: /azureml-envs/azureml_1b417bb747e35859ebf611fb43071e9c/lib/libcrypto.so.1.0.0: no version information available (required by ssh-keygen)\nssh-keygen: /azureml-envs/azureml_1b417bb747e35859ebf611fb43071e9c/lib/libcrypto.so.1.0.0: no version information available (required by ssh-keygen)\nbash: /azureml-envs/azureml_1b417bb747e35859ebf611fb43071e9c/lib/libtinfo.so.5: no version information available (required by bash)\nbash: /azureml-envs/azureml_1b417bb747e35859ebf611fb43071e9c/lib/libtinfo.so.5: no version information available (required by bash)\n\nStreaming azureml-logs/65_job_prep-tvmps_a908d1ef2dcb3755096ff07a574681d0b0400a48aae14b143627456f0ee35f11_d.txt\n===============================================================================================================\nbash: /azureml-envs/azureml_1b417bb747e35859ebf611fb43071e9c/lib/libtinfo.so.5: no version information available (required by bash)\nStarting job preparation. Current time:2020-03-18T21:31:32.959853\nExtracting the control code.\nfetching and extracting the control code on master node.\nRetrieving project from snapshot: 586b28b9-34f3-4933-8ccf-7e53ec9b93a9\nStarting the daemon thread to refresh tokens in background for process with pid = 88\nStarting project file download.\nFinished project file download.\nDownload from datastores if requested.\nAcquired lockfile /tmp/813385a4-ab91-452d-89a6-e58bbda14cbd-datastore.lock to downloading input data references\nDownload or mount from datasets if requested.\nJob preparation is complete. Current time:2020-03-18T21:31:36.437030\n\nStreaming azureml-logs/70_driver_log.txt\n========================================\nbash: /azureml-envs/azureml_1b417bb747e35859ebf611fb43071e9c/lib/libtinfo.so.5: no version information available (required by bash)\nbash: /azureml-envs/azureml_1b417bb747e35859ebf611fb43071e9c/lib/libtinfo.so.5: no version information available (required by bash)\nStarting the daemon thread to refresh tokens in background for process with pid = 140\nEntering Run History Context Manager.\n\nStreaming azureml-logs/75_job_post-tvmps_a908d1ef2dcb3755096ff07a574681d0b0400a48aae14b143627456f0ee35f11_d.txt\n===============================================================================================================\nbash: /azureml-envs/azureml_1b417bb747e35859ebf611fb43071e9c/lib/libtinfo.so.5: no version information available (required by bash)\nStarting job release. Current time:2020-03-18T21:31:48.980117\nLogging experiment finalizing status in history service.\nStarting the daemon thread to refresh tokens in background for process with pid = 166\nJob release is complete. Current time:2020-03-18T21:31:51.088236\n\nStepRun(compare.py) Execution Summary\n======================================\nStepRun( compare.py ) Status: Finished\n{'runId': '813385a4-ab91-452d-89a6-e58bbda14cbd', 'target': 'cpu-cluster', 'status': 'Completed', 'startTimeUtc': '2020-03-18T21:31:22.47263Z', 'endTimeUtc': '2020-03-18T21:32:01.272309Z', 'properties': {'azureml.runsource': 'azureml.StepRun', 'ContentSnapshotId': '586b28b9-34f3-4933-8ccf-7e53ec9b93a9', 'StepType': 'PythonScriptStep', 'ComputeTargetType': 'AmlCompute', 'azureml.pipelinerunid': '107dde6d-3721-477a-98bf-5f791b7bc78f', '_azureml.ComputeTargetType': 'amlcompute', 'ProcessInfoFile': 'azureml-logs/process_info.json', 'ProcessStatusFile': 'azureml-logs/process_status.json'}, 'inputDatasets': [], 'runDefinition': {'script': 'compare.py', 'useAbsolutePath': False, 'arguments': ['--compare_data1', '$AZUREML_DATAREFERENCE_workspaceblobstore_ede545fc', '--compare_data2', '$AZUREML_DATAREFERENCE_processed_data2', '--output_compare', '$AZUREML_DATAREFERENCE_processed_data3'], 'sourceDirectoryDataStore': None, 'framework': 'Python', 'communicator': 'None', 'target': 'cpu-cluster', 'dataReferences': {'workspaceblobstore_ede545fc': {'dataStoreName': 'workspaceblobstore', 'mode': 'Mount', 'pathOnDataStore': '20newsgroups/20news.pkl', 'pathOnCompute': None, 'overwrite': False}, 'processed_data2': {'dataStoreName': 'workspaceblobstore', 'mode': 'Mount', 'pathOnDataStore': 'azureml/8739d44e-4316-4b53-a97f-3a12717fb6ce/processed_data2', 'pathOnCompute': None, 'overwrite': False}, 'processed_data3': {'dataStoreName': 'workspaceblobstore', 'mode': 'Mount', 'pathOnDataStore': 'azureml/813385a4-ab91-452d-89a6-e58bbda14cbd/processed_data3', 'pathOnCompute': None, 'overwrite': False}}, 'data': {}, 'jobName': None, 'maxRunDurationSeconds': None, 'nodeCount': 1, 'environment': {'name': 'Experiment Data_dependency Environment', 'version': 'Autosave_2020-02-06T19:50:52Z_809dae5e', 'python': {'interpreterPath': 'python', 'userManagedDependencies': False, 'condaDependencies': {'channels': ['conda-forge'], 'dependencies': ['python=3.6.2', {'pip': ['azureml-defaults']}], 'name': 'azureml_1b417bb747e35859ebf611fb43071e9c'}, 'baseCondaEnvironment': None}, 'environmentVariables': {'EXAMPLE_ENV_VAR': 'EXAMPLE_VALUE'}, 'docker': {'baseImage': 'mcr.microsoft.com/azureml/base:intelmpi2018.3-ubuntu16.04', 'baseDockerfile': None, 'baseImageRegistry': {'address': None, 'username': None, 'password': None}, 'enabled': True, 'shmSize': '1g'}, 'spark': {'repositories': ['[]'], 'packages': [], 'precachePackages': True}, 'inferencingStackVersion': None}, 'history': {'outputCollection': True, 'directoriesToWatch': ['logs']}, 'spark': {'configuration': {'spark.app.name': 'Azure ML Experiment', 'spark.yarn.maxAppAttempts': '1'}}, 'amlCompute': {'name': None, 'vmSize': None, 'retainCluster': False, 'clusterMaxNodeCount': 1}, 'tensorflow': {'workerCount': 1, 'parameterServerCount': 1}, 'mpi': {'processCountPerNode': 1}, 'hdi': {'yarnDeployMode': 'Cluster'}, 'containerInstance': {'region': None, 'cpuCores': 2, 'memoryGb': 3.5}, 'exposedPorts': None, 'docker': {'useDocker': True, 'sharedVolumes': True, 'shmSize': '2g', 'arguments': []}, 'cmk8sCompute': {'configuration': {}}}, 'logFiles': {'azureml-logs/55_azureml-execution-tvmps_a908d1ef2dcb3755096ff07a574681d0b0400a48aae14b143627456f0ee35f11_d.txt': 'https://avadevitsmlsvc7139228118.blob.core.windows.net/azureml/ExperimentRun/dcid.813385a4-ab91-452d-89a6-e58bbda14cbd/azureml-logs/55_azureml-execution-tvmps_a908d1ef2dcb3755096ff07a574681d0b0400a48aae14b143627456f0ee35f11_d.txt?sv=2019-02-02&sr=b&sig=7sSqBU0irX2J2xvPDwFxJec4dAYuz7BNlHNrvpjfju0%3D&st=2020-03-18T21%3A22%3A07Z&se=2020-03-19T05%3A32%3A07Z&sp=r', 'azureml-logs/65_job_prep-tvmps_a908d1ef2dcb3755096ff07a574681d0b0400a48aae14b143627456f0ee35f11_d.txt': 'https://avadevitsmlsvc7139228118.blob.core.windows.net/azureml/ExperimentRun/dcid.813385a4-ab91-452d-89a6-e58bbda14cbd/azureml-logs/65_job_prep-tvmps_a908d1ef2dcb3755096ff07a574681d0b0400a48aae14b143627456f0ee35f11_d.txt?sv=2019-02-02&sr=b&sig=1udmvizVJHba2BEQI3zhtBHloUHIHILPzMvk%2F864e5k%3D&st=2020-03-18T21%3A22%3A07Z&se=2020-03-19T05%3A32%3A07Z&sp=r', 'azureml-logs/70_driver_log.txt': 'https://avadevitsmlsvc7139228118.blob.core.windows.net/azureml/ExperimentRun/dcid.813385a4-ab91-452d-89a6-e58bbda14cbd/azureml-logs/70_driver_log.txt?sv=2019-02-02&sr=b&sig=p82wQhCJXEtIHfMoijRBUcxDycvYM4cGUq%2BirvG6JVA%3D&st=2020-03-18T21%3A22%3A07Z&se=2020-03-19T05%3A32%3A07Z&sp=r', 'azureml-logs/75_job_post-tvmps_a908d1ef2dcb3755096ff07a574681d0b0400a48aae14b143627456f0ee35f11_d.txt': 'https://avadevitsmlsvc7139228118.blob.core.windows.net/azureml/ExperimentRun/dcid.813385a4-ab91-452d-89a6-e58bbda14cbd/azureml-logs/75_job_post-tvmps_a908d1ef2dcb3755096ff07a574681d0b0400a48aae14b143627456f0ee35f11_d.txt?sv=2019-02-02&sr=b&sig=jzjVnzf%2FWlCApqZDveOn4VZT%2B1B8eG4a9k5A94GOk7g%3D&st=2020-03-18T21%3A22%3A07Z&se=2020-03-19T05%3A32%3A07Z&sp=r', 'azureml-logs/process_info.json': 'https://avadevitsmlsvc7139228118.blob.core.windows.net/azureml/ExperimentRun/dcid.813385a4-ab91-452d-89a6-e58bbda14cbd/azureml-logs/process_info.json?sv=2019-02-02&sr=b&sig=EQPkjsUNT2lohn7WFkhYJ1xm2RuRHN8DN6rfNZ%2FVfVc%3D&st=2020-03-18T21%3A22%3A07Z&se=2020-03-19T05%3A32%3A07Z&sp=r', 'azureml-logs/process_status.json': 'https://avadevitsmlsvc7139228118.blob.core.windows.net/azureml/ExperimentRun/dcid.813385a4-ab91-452d-89a6-e58bbda14cbd/azureml-logs/process_status.json?sv=2019-02-02&sr=b&sig=8ZAI2zQylUtW7Q1vMC81cDR6RcXXTR39ypgpoHT8J%2FA%3D&st=2020-03-18T21%3A22%3A07Z&se=2020-03-19T05%3A32%3A07Z&sp=r', 'logs/azureml/140_azureml.log': 'https://avadevitsmlsvc7139228118.blob.core.windows.net/azureml/ExperimentRun/dcid.813385a4-ab91-452d-89a6-e58bbda14cbd/logs/azureml/140_azureml.log?sv=2019-02-02&sr=b&sig=lZjKN6wZq9emj352z7c98DmyN12RQm6zIf5%2BkbD%2F%2Bjg%3D&st=2020-03-18T21%3A22%3A07Z&se=2020-03-19T05%3A32%3A07Z&sp=r', 'logs/azureml/azureml.log': 'https://avadevitsmlsvc7139228118.blob.core.windows.net/azureml/ExperimentRun/dcid.813385a4-ab91-452d-89a6-e58bbda14cbd/logs/azureml/azureml.log?sv=2019-02-02&sr=b&sig=Q1O6TlHuramAyrEnm3fxbGjZhbkP1fOe1i8S4%2FfbhYg%3D&st=2020-03-18T21%3A22%3A07Z&se=2020-03-19T05%3A32%3A07Z&sp=r', 'logs/azureml/executionlogs.txt': 'https://avadevitsmlsvc7139228118.blob.core.windows.net/azureml/ExperimentRun/dcid.813385a4-ab91-452d-89a6-e58bbda14cbd/logs/azureml/executionlogs.txt?sv=2019-02-02&sr=b&sig=qraN82Gc8oNBH6r%2BgrH7jC%2FLcm2QRWrDDHWjSbbs9Ok%3D&st=2020-03-18T21%3A22%3A07Z&se=2020-03-19T05%3A32%3A07Z&sp=r', 'logs/azureml/stderrlogs.txt': 'https://avadevitsmlsvc7139228118.blob.core.windows.net/azureml/ExperimentRun/dcid.813385a4-ab91-452d-89a6-e58bbda14cbd/logs/azureml/stderrlogs.txt?sv=2019-02-02&sr=b&sig=pdVLPlg0OfgTM%2BBgokTXMbF%2BJxBo38CR1n7943Pq8X8%3D&st=2020-03-18T21%3A22%3A07Z&se=2020-03-19T05%3A32%3A07Z&sp=r', 'logs/azureml/stdoutlogs.txt': 'https://avadevitsmlsvc7139228118.blob.core.windows.net/azureml/ExperimentRun/dcid.813385a4-ab91-452d-89a6-e58bbda14cbd/logs/azureml/stdoutlogs.txt?sv=2019-02-02&sr=b&sig=WnNbsaC9FypVBPzuwUmmCP4OPG6K55lO0QHBDhDhjxg%3D&st=2020-03-18T21%3A22%3A07Z&se=2020-03-19T05%3A32%3A07Z&sp=r'}}\n\n\n\nPipelineRun Execution Summary\n==============================\nPipelineRun Status: Finished\n{'runId': '107dde6d-3721-477a-98bf-5f791b7bc78f', 'status': 'Completed', 'startTimeUtc': '2020-03-18T21:22:43.061093Z', 'endTimeUtc': '2020-03-18T21:32:07.281098Z', 'properties': {'azureml.runsource': 'azureml.PipelineRun', 'runSource': 'SDK', 'runType': 'SDK', 'azureml.parameters': '{}'}, 'inputDatasets': [], 'logFiles': {'logs/azureml/executionlogs.txt': 'https://avadevitsmlsvc7139228118.blob.core.windows.net/azureml/ExperimentRun/dcid.107dde6d-3721-477a-98bf-5f791b7bc78f/logs/azureml/executionlogs.txt?sv=2019-02-02&sr=b&sig=eDzk%2Bu37uCNhPQg%2Fz0EghhVEWLGmucT5EdYx%2BmXzyLQ%3D&st=2020-03-18T21%3A22%3A09Z&se=2020-03-19T05%3A32%3A09Z&sp=r', 'logs/azureml/stderrlogs.txt': 'https://avadevitsmlsvc7139228118.blob.core.windows.net/azureml/ExperimentRun/dcid.107dde6d-3721-477a-98bf-5f791b7bc78f/logs/azureml/stderrlogs.txt?sv=2019-02-02&sr=b&sig=jgtID9UyoMVEdON4P4QAAcKRpGt0%2F03ykGZGe%2BVaixk%3D&st=2020-03-18T21%3A22%3A09Z&se=2020-03-19T05%3A32%3A09Z&sp=r', 'logs/azureml/stdoutlogs.txt': 'https://avadevitsmlsvc7139228118.blob.core.windows.net/azureml/ExperimentRun/dcid.107dde6d-3721-477a-98bf-5f791b7bc78f/logs/azureml/stdoutlogs.txt?sv=2019-02-02&sr=b&sig=xMfQYVwPEJpuUzkp797ZaQcxnPY0zod5CDDHZ3bH9OM%3D&st=2020-03-18T21%3A22%3A09Z&se=2020-03-19T05%3A32%3A09Z&sp=r'}}\n\n"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "'Finished'"
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "pipeline_run1.wait_for_completion(show_output=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### See Outputs\n",
        "\n",
        "See where outputs of each pipeline step are located on your datastore.\n",
        "\n",
        "***Wait for pipeline run to complete, to make sure all the outputs are ready***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get Steps\n",
        "for step in pipeline_run1.get_steps():\n",
        "    print(\"Outputs of step \" + step.name)\n",
        "    \n",
        "    # Get a dictionary of StepRunOutputs with the output name as the key \n",
        "    output_dict = step.get_outputs()\n",
        "    \n",
        "    for name, output in output_dict.items():\n",
        "        \n",
        "        output_reference = output.get_port_data_reference() # Get output port data reference\n",
        "        print(\"\\tname: \" + name)\n",
        "        print(\"\\tdatastore: \" + output_reference.datastore_name)\n",
        "        print(\"\\tpath on datastore: \" + output_reference.path_on_datastore)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Download Outputs\n",
        "\n",
        "We can download the output of any step to our local machine using the SDK."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Retrieve the step runs by name 'train.py'\n",
        "train_step = pipeline_run1.find_step_run('train.py')\n",
        "\n",
        "if train_step:\n",
        "    train_step_obj = train_step[0] # since we have only one step by name 'train.py'\n",
        "    train_step_obj.get_output_data('processed_data1').download(\"./outputs\") # download the output to current directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Next: Publishing the Pipeline and calling it from the REST endpoint\n",
        "See this [notebook](https://aka.ms/pl-pub-rep) to understand how the pipeline is published and you can call the REST endpoint to run the pipeline."
      ]
    }
  ],
  "metadata": {
    "authors": [
      {
        "name": "sanpil"
      }
    ],
    "category": "tutorial",
    "compute": [
      "AML Compute"
    ],
    "datasets": [
      "Custom"
    ],
    "deployment": [
      "None"
    ],
    "exclude_from_index": false,
    "framework": [
      "Azure ML"
    ],
    "friendly_name": "Azure Machine Learning Pipelines with Data Dependency",
    "kernelspec": {
      "display_name": "Python 3.6.2 64-bit ('deal_aml85': conda)",
      "language": "python",
      "name": "python36264bitdealaml85condaf59d2bba2d6c43f78349ea5147e8c8b7"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.2-final"
    },
    "order_index": 2,
    "star_tag": [
      "featured"
    ],
    "tags": [
      "None"
    ],
    "task": "Demonstrates how to construct a Pipeline with data dependency between steps"
  },
  "nbformat": 4,
  "nbformat_minor": 2
}